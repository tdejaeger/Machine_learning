{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\huge \\textbf{Outlier detection}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use different methods to detect points that display an anomalous behaviour. First I will show three unsupervised outlier detection using: Multivariate Gaussian model (3 sigma clipping and EllipticEnvelope), Local Outlier Factor, and the Isolation Forest methods.Second, I will show you two supervised methods: Support Vector Machine and Random Forest Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{1.) Unsupervised Outlier Detection using Multivariate Gaussian Model}$\n",
    "\n",
    "One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the “shape” of the data (1,2,3 sigma confidence levels), and can define outlying observations as observations which stand far enough from the fit shape (3 sigma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import package and figure properties\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "plt.rcParams['legend.numpoints']=1\n",
    "plt.rcParams['xtick.major.size'] = 11\n",
    "plt.rcParams['xtick.minor.size'] = 5\n",
    "plt.rcParams['ytick.major.size'] = 11\n",
    "plt.rcParams['ytick.minor.size'] = 5\n",
    "plt.rcParams['xtick.minor.visible']=True #See minor tick\n",
    "plt.rcParams['text.usetex']=True #use Latex\n",
    "plt.rcParams['axes.linewidth']=2 #width axes\n",
    "plt.rcParams['axes.labelsize']=25 #\n",
    "plt.rcParams['ytick.labelsize']=22 #fontsize of tick labels\n",
    "plt.rcParams['xtick.labelsize']=22 #fontsize of tick labels\n",
    "plt.rcParams['ytick.direction']='inout' ## direction: in, out, or inout\n",
    "plt.rcParams['xtick.direction']='inout' ## direction: in, out, or inout\n",
    "\n",
    "plt.rcParams['xtick.major.top']=True #draw x axis top major ticks\n",
    "plt.rcParams['xtick.major.bottom']=True #draw x axis bottom major ticks\n",
    "plt.rcParams['xtick.minor.top']=True ## draw x axis top minor ticks\n",
    "plt.rcParams['xtick.minor.bottom']=True #draw x axis bottom minor ticks\n",
    "\n",
    "plt.rcParams['ytick.major.left']=True #draw y axis left major ticks\n",
    "plt.rcParams['ytick.major.right']=True #draw y axis right major ticks\n",
    "plt.rcParams['ytick.minor.left']=True ## draw y axis left minor ticks\n",
    "plt.rcParams['ytick.minor.right']=True #draw y axis right minor ticks\n",
    "\n",
    "\n",
    "plt.rcParams['font.weight']='bold'\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['axes.titlesize']=22\n",
    "plt.rcParams['figure.titlesize']=22\n",
    "\n",
    "plt.rcParams['text.latex.preamble']=[r'\\boldmath']\n",
    "\n",
    "def check_model(TN,FP,FN,TP):\n",
    "    #True Negative, False Positive, False Negative,rue Positive\n",
    "    '''\n",
    "    Accuracy: Accuracy is the most intuitive performance measure and it is simply a \n",
    "    ratio of correctly predicted observation to the total observations. \n",
    "    One may think that, if we have high accuracy then our model is best. \n",
    "    Yes, accuracy is a great measure but only when you have symmetric datasets \n",
    "    where values of false positive and false negatives are almost same. \n",
    "    Therefore, you have to look at other parameters to evaluate the performance of your model. \n",
    "    \n",
    "    '''    \n",
    "    accuracy=(TP+TN)/(TP+FN+FP+TN)\n",
    "    \n",
    "    '''\n",
    "    Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive\n",
    "    observations. The question that this metric answer is of all passengers that labeled as survived, \n",
    "    how many actually survived? High precision relates to the low false positive rate. \n",
    "    '''    \n",
    "    precision = TP/(TP+FP)\n",
    "    \n",
    "    '''\n",
    "    Recall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations\n",
    "    in actual class - yes.\n",
    "    '''\n",
    "    recall= TP/(TP+FN)\n",
    "    \n",
    "    '''\n",
    "    F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives \n",
    "    and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually \n",
    "    more useful than accuracy, especially if you have an uneven class distribution. \n",
    "    Accuracy works best if false positives and false negatives have similar cost. \n",
    "    If the cost of false positives and false negatives are very different, it’s better to look at both \n",
    "    Precision and Recall\n",
    "    '''\n",
    "    F1_score = 2*(recall * precision) / (recall + precision)\n",
    "     \n",
    "    print(f'Accuracy = {accuracy:.4f}')\n",
    "    print(f'Precision = {precision:.4f}')\n",
    "    print(f'Recall = {recall:.4f}')\n",
    "    print(f'F1 score = {F1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data\n",
    "size=500\n",
    "mean= np.array([0.1, 0.2])\n",
    "cov = np.array([[0.01, 0.005], [0.005, 0.01]])\n",
    "\n",
    "data= np.random.multivariate_normal(mean, cov, size=size)\n",
    "X_data,Y_data=data[:, 0], data[:, 1]\n",
    "\n",
    "# outliers\n",
    "mean_out = np.array([0.15, 0.3])\n",
    "cov_out = np.array([[0.1, 0], [0, 0.1]])\n",
    "\n",
    "data_out = np.random.multivariate_normal(mean_out, cov_out, size=int(size/10)) # only 1/10 are outliers\n",
    "X_out,Y_out=data_out[:,0],data_out[:,1]\n",
    "\n",
    "#Figure\n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "\n",
    "ax1.plot(X_data,Y_data,'bo',alpha=0.5,label=r'\\textbf{Data}')\n",
    "ax1.plot(X_out, Y_out,'ro',alpha=0.5,label=r'\\textbf{Outliers}')\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=0,title='',markerscale=0.5,ncol=1,prop={'size':12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost the data are grouped together (blue) while a 50 data points are randomly displayed. To detect the outliers as we use the multivariate Gaussian method, we need to derive first the mean ($\\mu$) and the standard deviation $\\sigma$. Note the covariance matrix diagonal gives the n variances (squares of standard deviations) for the n variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to estimate parameters of a multivariate normal distribution based on the data\n",
    "def estimate(X):\n",
    "    \"\"\"Calculates estimates of the means mu and variance sigma2 of the data X which contains several features.\"\"\"\n",
    "    mu = np.mean(X)\n",
    "    sigma = np.std(X)\n",
    "    return mu, sigma\n",
    "\n",
    "#Regroup all the data\n",
    "X_tot=np.concatenate((X_data,X_out))\n",
    "Y_tot=np.concatenate((Y_data,Y_out))\n",
    "points_tot=np.vstack((X_tot,Y_tot)).T\n",
    "\n",
    "#We know\n",
    "n_outliers = len(X_out)\n",
    "target = np.ones(len(points_tot), dtype=int)\n",
    "target[-n_outliers:] = -1\n",
    "\n",
    "mu_X, sigma_X=estimate(X_tot)\n",
    "mu_Y,sigma_Y =estimate(Y_tot)\n",
    "print(\"The mean of X\", mu_X)\n",
    "print(\"The standard deviation of X\", sigma_X)\n",
    "print(\"The mean of Y\", mu_Y)\n",
    "print(\"The standard deviation of Y\", sigma_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's define and plot the 1,2,3 sigma contours (60,95,99.7) of the data (including outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_cov_ellipse(points, nstd=2, ax=None, **kwargs):\n",
    "    pos = points.mean(axis=0)\n",
    "    cov = np.cov(points, rowvar=False)\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height =  2* nstd * np.sqrt(vals)\n",
    "    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, **kwargs)\n",
    "    ax.add_patch(ellip)\n",
    "    return ellip\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "#Figure\n",
    "fig,ax1 = plt.subplots(figsize=(6,6), facecolor='w', edgecolor='k')\n",
    "ax1.plot(X_data,Y_data,'bo',alpha=0.5,ms=3,label=r'\\textbf{Data}')\n",
    "ax1.plot(X_out, Y_out,'ro',alpha=0.5,ms=3,label=r'\\textbf{Outliers}')\n",
    "\n",
    "plot_cov_ellipse(points_tot, nstd=1, ax=ax1,edgecolor='k', fc='None', lw=3,zorder=3,label=r'\\textbf{1$\\sigma$}')\n",
    "plot_cov_ellipse(points_tot, nstd=2, ax=ax1,edgecolor='k',linestyle='--', fc='None', lw=3,zorder=3,label=r'\\textbf{2$\\sigma$}')\n",
    "plot_cov_ellipse(points_tot, nstd=3, ax=ax1,edgecolor='k', linestyle=':',fc='None', lw=3,zorder=3,label=r'\\textbf{3$\\sigma$}')\n",
    "\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=0,title='',markerscale=0.5,ncol=1,prop={'size':12})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Astronomy, we generally use a 3-sigma cut to remove the outliers. Therefore all the point situated outside the 3-sigma ellipse are considered as outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def point_out_ellipse(points, nstd=3):\n",
    "    pos = points.mean(axis=0)\n",
    "    cov = np.cov(points, rowvar=False)\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    theta=-theta*np.pi/180\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height = nstd * np.sqrt(vals)\n",
    "    \n",
    "    value=(((points[:,0]-pos[0])*np.cos(theta)+(points[:,1]-pos[1])*np.sin(theta))/height)**2+(((points[:,0]-pos[0])*np.sin(theta)-(points[:,1]-pos[1])*np.cos(theta))/width)**2\n",
    "    return points[value>1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nstd=3\n",
    "\n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "ax1.plot(X_data,Y_data,'bo',alpha=0.5,ms=5,label=r'\\textbf{Data}')\n",
    "ax1.plot(X_out, Y_out,'ro',alpha=0.5,ms=5,label=r'\\textbf{Outliers}')\n",
    "\n",
    "plot_cov_ellipse(points_tot, nstd=1, ax=ax1,edgecolor='k', fc='None', lw=3,zorder=3,label=r'\\textbf{1$\\sigma$}')\n",
    "plot_cov_ellipse(points_tot, nstd=2, ax=ax1,edgecolor='k',linestyle='--', fc='None', lw=3,zorder=3,label=r'\\textbf{2$\\sigma$}')\n",
    "plot_cov_ellipse(points_tot, nstd=3, ax=ax1,edgecolor='k', linestyle=':',fc='None', lw=3,zorder=3,label=r'\\textbf{3$\\sigma$}')\n",
    "\n",
    "ax1.plot(point_out_ellipse(points_tot,nstd).T[0],point_out_ellipse(points_tot,nstd).T[1],color='g',marker='o',markerfacecolor='None',markeredgecolor='green',mew=2,markersize=10,linestyle='None',label=r'\\textbf{%s$\\sigma$ cut}'%nstd)\n",
    "\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=0,title='',markerscale=0.5,ncol=2,prop={'size':12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "x = (point_out_ellipse(points_tot,nstd)[:, None] == data_out).all(-1)\n",
    "tot_removed=np.shape(point_out_ellipse(points_tot,nstd))[0]\n",
    "print('###### Multivariate Gaussian Model ######')\n",
    "print(r\"Number of outliers well removed %s/%s with %s-sigma cut\"%(np.size(x[x==True]),np.size(X_out),nstd))\n",
    "print(r\"Number of data removed %s/%s with %s-sigma cut\"%(tot_removed-np.size(x[x==True]),np.size(X_data),nstd))\n",
    "\n",
    "print('######Classification metric for Multivariate Gaussian Model: ##########')\n",
    "check_model(np.size(x[x==True]),np.size(X_out)-np.size(x[x==True]),(tot_removed-np.size(x[x==True])),np.size(X_data)-(tot_removed-np.size(x[x==True])))#TN FP FN TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a 3-sigma cut, we remove the majority of the outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{2.) Unsupervised Outlier Detection using EllipticEnvelope (Elli)}$:\n",
    "\n",
    "EllipticEnvelope which can also be unsupervised, fits the tightest Gaussianthat it can while discarding some fixed fraction of contamination points set by the user. This method is better than the previous used as it allows us to include the amount of contamination of the data. Therefore, the Gaussian distribution will be more similar to the real data (more elliptical than previously)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EllipticEnvelope\n",
    "import scipy.stats as stats\n",
    "\n",
    "outlier_frac = 0.10 #10% of outliers\n",
    "elli = EllipticEnvelope(contamination=outlier_frac)\n",
    "elli.fit(points_tot)\n",
    "y_pred_elli = elli.predict(points_tot)\n",
    "\n",
    "xx, yy = np.meshgrid(np.linspace(np.min(points_tot[:,0])-0.1, np.max(points_tot[:,0])+0.1, 200), np.linspace(np.min(points_tot[:,1])-0.1, np.max(points_tot[:,1])+0.1, 200))\n",
    "Z = elli.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "threshold = stats.scoreatpercentile(elli.decision_function(points_tot), 100*outlier_frac)\n",
    "print(r\"The threshold for being an outlier is %s\"%int(threshold))\n",
    "\n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "ax1.plot(X_data,Y_data,'bo',alpha=0.5,ms=5,label=r'\\textbf{Data}')\n",
    "ax1.plot(X_out, Y_out,'ro',alpha=0.5,ms=5,label=r'\\textbf{Outliers}')\n",
    "cont=ax1.contourf(xx, yy, Z,alpha=0.5,cmap=plt.cm.seismic) \n",
    "cax = plt.axes([0.95, 0.120, 0.03, 0.76])\n",
    "clb = plt.colorbar(cont,cax=cax)\n",
    "clb.ax.set_ylabel(r'\\textbf{Shifted Mahalanobis distances}', labelpad=30, rotation=270,fontsize=plt.rcParams['axes.labelsize'])\n",
    "clb.ax.tick_params(labelsize=18)\n",
    "\n",
    "plot_cov_ellipse(points_tot, nstd=1, ax=ax1,edgecolor='k', fc='None', lw=3,zorder=3,label=r'\\textbf{1$\\sigma$}')\n",
    "plot_cov_ellipse(points_tot, nstd=2, ax=ax1,edgecolor='k',linestyle='--', fc='None', lw=3,zorder=3,label=r'\\textbf{2$\\sigma$}')\n",
    "plot_cov_ellipse(points_tot, nstd=3, ax=ax1,edgecolor='k', linestyle=':',fc='None', lw=3,zorder=3,label=r'\\textbf{3$\\sigma$}')\n",
    "\n",
    "ax1.plot(point_out_ellipse(points_tot,nstd).T[0],point_out_ellipse(points_tot,nstd).T[1],color='g',marker='o',markerfacecolor='None',markeredgecolor='green',mew=2,markersize=8,linestyle='None',label=r'\\textbf{%s$\\sigma$ cut}'%nstd)\n",
    "ax1.plot(points_tot[:,0][y_pred_elli==-1],points_tot[:,1][y_pred_elli==-1],color='k',marker='o',markerfacecolor='None',markeredgecolor='black',mew=2,markersize=14,linestyle='None',label=r'\\textbf{Elli}')\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=0,title='',markerscale=0.5,ncol=2,prop={'size':12})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_elli = np.size(y_pred_elli[-50:][y_pred_elli[-50:]==-1])\n",
    "tot_rem_elli=np.size(y_pred_elli[y_pred_elli==-1])\n",
    "print('####### EllipticEnvelope #######')\n",
    "print(r\"Number of outliers well removed %s/%s with EllipticEnvelope\"%(x_elli,np.size(X_out)))\n",
    "print(r\"Number of data  removed %s/%s with EllipticEnvelope\"%(tot_rem_elli-x_elli,np.size(X_data)))\n",
    "\n",
    "print('######Classification metric for EllipticEnvelope: ##########')\n",
    "\n",
    "print(f'Accuracy = {accuracy_score(target, y_pred_elli):.4f}')\n",
    "print(f'Precision = {precision_score(target, y_pred_elli):.4f}')\n",
    "print(f'Recall = {recall_score(target, y_pred_elli):.4f}')\n",
    "print(f'F1 score = {f1_score(target, y_pred_elli):.4f}')\n",
    "print(f'ROC AUC score = {roc_auc_score(target, y_pred_elli)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{3.) Unsupervised Outlier Detection using Local Outlier Factor (LOF)}$:\n",
    "\n",
    "The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# fit the model for outlier detection (default)\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n",
    "# use fit_predict to compute the predicted labels of the training samples\n",
    "# (when LOF is used for outlier detection, the estimator has no predict,\n",
    "# decision_function and score_samples methods).\n",
    "y_pred_lof = lof.fit_predict(points_tot)\n",
    "\n",
    "X_scores_lof = lof.negative_outlier_factor_\n",
    "radius = (X_scores_lof.max() - X_scores_lof) / (X_scores_lof.max() - X_scores_lof.min())\n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "ax1.plot(X_data,Y_data,'bo',alpha=0.5,ms=5,label=r'\\textbf{Data}')\n",
    "ax1.plot(X_out, Y_out,'ro',alpha=0.5,ms=5,label=r'\\textbf{Outliers}')\n",
    "\n",
    "#plot_cov_ellipse(points_tot, nstd=1, ax=ax1,edgecolor='k', fc='None', lw=3,zorder=3,label=r'\\textbf{1$\\sigma$}')\n",
    "#plot_cov_ellipse(points_tot, nstd=2, ax=ax1,edgecolor='k',linestyle='--', fc='None', lw=3,zorder=3,label=r'\\textbf{2$\\sigma$}')\n",
    "#plot_cov_ellipse(points_tot, nstd=3, ax=ax1,edgecolor='k', linestyle=':',fc='None', lw=3,zorder=3,label=r'\\textbf{3$\\sigma$}')\n",
    "ax1.scatter(points_tot[:, 0], points_tot[:, 1], s=1000 * radius, edgecolors='g',facecolors='none', label='Outlier scores')\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=0,title='',markerscale=0.5,ncol=2,prop={'size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Let's put a cut in the score: mean score+ 1sigma\n",
    "Outlier_LOF=points_tot[radius>np.mean(radius)+np.std(radius)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "ax1.plot(X_data,Y_data,'bo',alpha=0.5,ms=5,label=r'\\textbf{Data}')\n",
    "ax1.plot(X_out, Y_out,'ro',alpha=0.5,ms=5,label=r'\\textbf{Outliers}')\n",
    "ax1.plot(Outlier_LOF[:,0], Outlier_LOF[:,1],color='g',marker='o',markerfacecolor='None',markeredgecolor='green',mew=2,markersize=8,linestyle='None',label=r'\\textbf{Outliers LOF threshold}')\n",
    "\n",
    "ax1.plot(points_tot[:,0][y_pred_lof==-1], points_tot[:,1][y_pred_lof==-1],color='k',marker='o',markerfacecolor='None',markeredgecolor='black',mew=2,markersize=13,linestyle='None',label=r'\\textbf{Outliers LOF}')\n",
    "\n",
    "\n",
    "#plot_cov_ellipse(points_tot, nstd=1, ax=ax1,edgecolor='k', fc='None', lw=3,zorder=3,label=r'\\textbf{1$\\sigma$}')\n",
    "#plot_cov_ellipse(points_tot, nstd=2, ax=ax1,edgecolor='k',linestyle='--', fc='None', lw=3,zorder=3,label=r'\\textbf{2$\\sigma$}')\n",
    "#plot_cov_ellipse(points_tot, nstd=3, ax=ax1,edgecolor='k', linestyle=':',fc='None', lw=3,zorder=3,label=r'\\textbf{3$\\sigma$}')\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=0,title='',markerscale=0.5,ncol=2,prop={'size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lof_1sig = (Outlier_LOF[:, None] == data_out).all(-1)\n",
    "tot_rem_lof_1sig=np.shape(Outlier_LOF[:, None])[0]\n",
    "\n",
    "print('####### LOF and threshold of 1 sigma #######')\n",
    "print(r\"Number of outliers well removed %s/%s with 1-sigma cut\"%(np.size(x_lof_1sig[x_lof_1sig==True]),np.size(X_out)))\n",
    "print(r\"Number of data  removed %s/%s with 1-sigma cut\"%(tot_rem_lof_1sig-np.size(x_lof_1sig[x_lof_1sig==True]),np.size(X_data)))\n",
    "print('######Classification metric for LOF and threshold: ##########')\n",
    "check_model(np.size(x_lof_1sig[x_lof_1sig==True]),np.size(X_out)-np.size(x_lof_1sig[x_lof_1sig==True]),tot_rem_lof_1sig-np.size(x_lof_1sig[x_lof_1sig==True]),np.size(X_data)-(tot_rem_lof_1sig-np.size(x_lof_1sig[x_lof_1sig==True])))#TN FP FN TP\n",
    "x_lof = np.size(y_pred_lof[-50:][y_pred_lof[-50:]==-1])\n",
    "tot_rem_lof=np.size(y_pred_lof[y_pred_lof==-1])\n",
    "print('####### LOF #######')\n",
    "print(r\"Number of outliers well removed %s/%s with LOF\"%(x_lof,np.size(X_out)))\n",
    "print(r\"Number of data  removed %s/%s with LOF\"%(tot_rem_lof-x_lof,np.size(X_data)))\n",
    "\n",
    "print('######Classification metric for LOF: ##########')\n",
    "\n",
    "print(f'Accuracy = {accuracy_score(target, y_pred_lof):.4f}')\n",
    "print(f'Precision = {precision_score(target, y_pred_lof):.4f}')\n",
    "print(f'Recall = {recall_score(target, y_pred_lof):.4f}')\n",
    "print(f'F1 score = {f1_score(target, y_pred_lof):.4f}')\n",
    "print(f'ROC AUC score = {roc_auc_score(target, y_pred_lof)*100:.2f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{4.) Unsupervised Outlier Detection using Isolation Forest (IF)}$:\n",
    "\n",
    "The IsolationForest ‘isolates’ observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature.\n",
    "\n",
    "Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node.\n",
    "\n",
    "This path length, averaged over a forest of such random trees, is a measure of normality and our decision function.\n",
    "\n",
    "Random partitioning produces noticeable shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Isolation Forests\n",
    "# contamination: percentage of outliers\n",
    "clf = IsolationForest(max_samples=200, behaviour='new',contamination='auto')\n",
    "clf.fit(points_tot)\n",
    "y_pred_if = clf.fit_predict(points_tot)\n",
    "\n",
    "\n",
    "# plot decision boundries \n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "#create grid of points\n",
    "xx, yy = np.meshgrid(np.linspace(np.min(points_tot[:,0])-0.1, np.max(points_tot[:,0])+0.1, 200), np.linspace(np.min(points_tot[:,1])-0.1, np.max(points_tot[:,1])+0.1, 200))\n",
    "\n",
    "Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cont=ax1.contourf(xx, yy, Z,7,cmap=plt.cm.seismic) #note for contour values: cont.allsegs\n",
    "\n",
    "#Color bar\n",
    "cax = plt.axes([0.95, 0.120, 0.03, 0.76])\n",
    "clb = plt.colorbar(cont,cax=cax)\n",
    "clb.ax.set_ylabel(r'\\textbf{Average anomaly score}', labelpad=30, rotation=270,fontsize=plt.rcParams['axes.labelsize'])\n",
    "clb.ax.tick_params(labelsize=18)\n",
    "\n",
    "\n",
    "# plot the acutal objects\n",
    "ax1.plot(points_tot[:, 0], points_tot[:, 1], color='k',marker='o',alpha=0.7,ms=5,linestyle='None')\n",
    "\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By eye, we can define a threshold below which an object will be defined as an outlier. Here I select: -0.05, it means that all the data in the darkblue,blue, gray will be considered as outliers while the data in the pink, red, and darkred will be considered as the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "threshold=-0.05\n",
    "\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "pt = np.array( (xx.flatten(), yy.flatten()) ).T\n",
    "val = Z.flatten()\n",
    "\n",
    "# We will sample the grid at the point location\n",
    "Z0 = griddata( pt, val, (points_tot[:,0],points_tot[:,1]))\n",
    "#To see if a point has a value <0.05,\n",
    "outlier_IF=points_tot[Z0<threshold]\n",
    "\n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "ax1.plot(X_data,Y_data,'bo',alpha=0.5,ms=5,label=r'\\textbf{Data}')\n",
    "ax1.plot(X_out, Y_out,'ro',alpha=0.5,ms=5,label=r'\\textbf{Outliers}')\n",
    "\n",
    "ax1.plot(outlier_IF[:,0], outlier_IF[:,1],color='g',marker='o',markerfacecolor='None',markeredgecolor='green',mew=2,markersize=8,linestyle='None',label=r'\\textbf{Threshold IF}')\n",
    "cont=ax1.contourf(xx, yy, Z,7,cmap=plt.cm.seismic) #note for contour values: cont.allsegs\n",
    "\n",
    "ax1.plot(points_tot[:,0][y_pred_if==-1], points_tot[:,1][y_pred_if==-1],color='k',marker='o',markerfacecolor='None',markeredgecolor='black',mew=2,markersize=13,linestyle='None',label=r'\\textbf{Outliers IF}')\n",
    "\n",
    "\n",
    "#Color bar\n",
    "cax = plt.axes([0.95, 0.120, 0.03, 0.76])\n",
    "clb = plt.colorbar(cont,cax=cax)\n",
    "clb.ax.set_ylabel(r'\\textbf{Average anomaly score}', labelpad=30, rotation=270,fontsize=plt.rcParams['axes.labelsize'])\n",
    "clb.ax.tick_params(labelsize=18)\n",
    "\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=4,title='',markerscale=0.5,ncol=2,prop={'size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_out_real = (outlier_IF[:, None] == data_out).all(-1)\n",
    "#bad remove= total removed- good removed\n",
    "n_bad_rem=np.shape(outlier_IF[:,0])[0]-np.size(x_out_real[x_out_real==True])\n",
    "\n",
    "print('####### IF and threshold of 1 sigma #######')\n",
    "print(r\"Number of outliers well removed %s/%s with threshold\"%(np.size(x_out_real[x_out_real==True]),np.size(X_out)))\n",
    "print(r\"Number of data  removed %s/%s with 1-sigma threshold\"%(n_bad_rem,np.size(X_data)))\n",
    "print('######Classification metric for IF+ Threshold: ##########')\n",
    "check_model(np.size(x_out_real[x_out_real==True]),np.size(X_out)-np.size(x_out_real[x_out_real==True]),n_bad_rem,np.size(X_data)-n_bad_rem)#TN FP FN TP\n",
    "\n",
    "x_if = np.size(y_pred_if[-50:][y_pred_if[-50:]==-1])\n",
    "tot_rem_if=np.size(y_pred_if[y_pred_if==-1])\n",
    "\n",
    "print('####### IF #######')\n",
    "print(r\"Number of outliers well removed %s/%s with IF\"%(x_if,np.size(X_out)))\n",
    "print(r\"Number of data  removed %s/%s with IF\"%(tot_rem_if-x_if,np.size(X_data)))\n",
    "\n",
    "print('######Classification metric for IF: ##########')\n",
    "\n",
    "print(f'Accuracy = {accuracy_score(target, y_pred_if):.2f}')\n",
    "print(f'Precision = {precision_score(target, y_pred_if):.2f}')\n",
    "print(f'Recall = {recall_score(target, y_pred_if):.2f}')\n",
    "print(f'F1 score = {f1_score(target, y_pred_if):.2f}')\n",
    "print(f'ROC AUC score = {roc_auc_score(target, y_pred_if)*100:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "$\\Large \\textbf{5.) Supervised Outlier Detection using Support Vector Machine (SVM)}$:\n",
    "\n",
    "SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane which separates the data into classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "import pandas as pd \n",
    "# data+outliers = points_tot\n",
    "# Because it is a supervised machine learning, we need labels: target= 1 for data, -1 for outliers\n",
    "#put data in a dataframe\n",
    "df = pd.DataFrame(points_tot, columns=['X', 'Y'])\n",
    "df['class'] = target\n",
    "#filters for data based on Y\n",
    "real = df['class'] == 1\n",
    "false = df['class'] == -1\n",
    "# Let's split the data in random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(df[['X', 'Y']], df['class'], test_size = 0.3, random_state = 0)\n",
    "\n",
    "# train a one-class SVM. Start with some hyper parameters\n",
    "#clf = svm.OneClassSVM(nu=0.1, kernel=\"rbf\", gamma=0.01)\n",
    "#Training of SVM with RBF (radial basis function) kernel\n",
    "#SVC\n",
    "svc=svm.SVC(C=100, kernel='rbf',gamma='auto', probability=True)\n",
    "svc.fit(xTrain,yTrain)\n",
    "\n",
    "#OneclassSVM\n",
    "oneClass=svm.OneClassSVM(nu=0.07, kernel='rbf',gamma='auto')\n",
    "oneClass.fit(xTrain,yTrain)\n",
    "\n",
    "# now we can predict for all the objects whether they belong to the class (1) or not (-1).\n",
    "y_pred_train_svc = svc.predict(xTrain)\n",
    "y_pred_test_svc = svc.predict(xTest)\n",
    "y_pred_tot_svc=svc.predict(points_tot)\n",
    "\n",
    "y_pred_train_oneClass = oneClass.predict(xTrain)\n",
    "y_pred_test_oneClass = oneClass.predict(xTest)\n",
    "y_pred_tot_oneClass=oneClass.predict(points_tot)\n",
    "\n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "ax1.plot(df['X'][real],df['Y'][real],'bo',alpha=0.5,ms=5,label=r'\\textbf{Data}',zorder=1)\n",
    "ax1.plot(df['X'][false], df['Y'][false],'ro',alpha=0.5,ms=5,label=r'\\textbf{Outliers}',zorder=1)\n",
    "\n",
    "ax1.plot(df['X'][y_pred_tot_svc==-1], df['Y'][y_pred_tot_svc==-1],color='g',marker='o',markerfacecolor='None',markeredgecolor='green',mew=2,markersize=8,linestyle='None',label=r'\\textbf{Outliers SVC}',zorder=1)\n",
    "\n",
    "ax1.plot(df['X'][y_pred_tot_oneClass==-1], df['Y'][y_pred_tot_oneClass==-1],color='k',marker='o',markerfacecolor='None',markeredgecolor='black',mew=2,markersize=14,linestyle='None',label=r'\\textbf{Outliers Oneclass}',zorder=2)\n",
    "\n",
    "\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=0,title='',markerscale=0.5,ncol=2,prop={'size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out_rem=np.size(y_pred_tot_svc[-50:][y_pred_tot_svc[-50:]==-1])\n",
    "n_tot_out=np.size(y_pred_tot_svc[y_pred_tot_svc==-1])\n",
    "print('##### SVC #####')\n",
    "print(r\"Using SVM and SVC we remove %s/%s of the outliers \"%(n_out_rem,np.size(X_out)))\n",
    "print(r\"Using SVM and SVC we remove %s/%s of the real data\"%((n_tot_out-n_out_rem),np.size(X_data)))\n",
    "\n",
    "print('######Classification metric for SVC: ##########')\n",
    "\n",
    "print(f'Accuracy = {accuracy_score(target, y_pred_tot_svc):.2f}')\n",
    "print(f'Precision = {precision_score(target, y_pred_tot_svc):.2f}')\n",
    "print(f'Recall = {recall_score(target, y_pred_tot_svc):.2f}')\n",
    "print(f'F1 score = {f1_score(target, y_pred_tot_svc):.2f}')\n",
    "print(f'ROC AUC score = {roc_auc_score(target, y_pred_tot_svc)*100:.2f}')\n",
    "\n",
    "\n",
    "n_out_rem_oneclass=np.size(y_pred_tot_oneClass[-50:][y_pred_tot_oneClass[-50:]==-1])\n",
    "n_tot_out_oneclass=np.size(y_pred_tot_oneClass[y_pred_tot_oneClass==-1])\n",
    "\n",
    "print('##### OneclassSVM #####')\n",
    "print(r\"Using SVM and Oneclass we remove %s/%s of the outliers \"%(n_out_rem_oneclass,np.size(X_out)))\n",
    "print(r\"Using SVM and Oneclass we remove %s/%s of the real data\"%((n_tot_out_oneclass-n_out_rem_oneclass),np.size(X_data)))\n",
    "\n",
    "print('######Classification metric for OneclassSVM: ##########')\n",
    "\n",
    "print(f'Accuracy = {accuracy_score(target, y_pred_tot_oneClass):.2f}')\n",
    "print(f'Precision = {precision_score(target, y_pred_tot_oneClass):.2f}')\n",
    "print(f'Recall = {recall_score(target, y_pred_tot_oneClass):.2f}')\n",
    "print(f'F1 score = {f1_score(target, y_pred_tot_oneClass):.2f}')\n",
    "print(f'ROC AUC score = {roc_auc_score(target, y_pred_tot_oneClass)*100:.2f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to plot the probability to be real data using SVC:\n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "\n",
    "cs = ax1.scatter(df['X'], df['Y'], c=svc.predict_proba(points_tot)[:,1], cmap='RdBu_r')\n",
    "\n",
    "cax = plt.axes([0.95, 0.120, 0.03, 0.76])\n",
    "clb = plt.colorbar(cs,cax=cax)\n",
    "clb.ax.set_ylabel(r'\\textbf{Probability Real Data}', labelpad=30, rotation=270,fontsize=plt.rcParams['axes.labelsize'])\n",
    "clb.ax.tick_params(labelsize=18)\n",
    "\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large \\textbf{6.) Supervised Outlier Detection using Random Forest Classifier (RF)}$:\n",
    "\n",
    "A random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# train a supervised RF to separate between the two groups\n",
    "# number of trees: 50\n",
    "XTrain, XTest, yTrain, yTest = train_test_split(df[['X', 'Y']], df['class'], test_size = 0.2, random_state = 0)\n",
    "rfc = RandomForestClassifier(n_estimators=50)\n",
    "rfc.fit(XTrain, yTrain)\n",
    "# let's examine the prediction of the RF on the training and the test set\n",
    "y_train_pred = rfc.predict(XTrain)\n",
    "y_test_pred = rfc.predict(XTest)\n",
    "\n",
    "# get the classification accuracy on the training and the test set\n",
    "score_train = rfc.score(XTrain, yTrain)\n",
    "score_test = rfc.score(XTest, yTest)\n",
    "print ('accuracy on training set: ', score_train)\n",
    "print ('accuracy on test set: ', score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first create the decision boundries of the RF - this corresponds to the probability of the prediction\n",
    "Z_rfc = rfc.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z_rfc = Z_rfc.reshape(xx.shape)\n",
    "\n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "ax1.plot(X_data,Y_data,'bo',alpha=0.5,ms=5,label=r'\\textbf{Data}')\n",
    "ax1.plot(X_out, Y_out,'ro',alpha=0.5,ms=5,label=r'\\textbf{Outliers}')\n",
    "\n",
    "cont=ax1.contourf(xx, yy, Z_rfc,7,alpha=0.4,cmap=plt.cm.seismic) #note for contour values: cont.allsegs\n",
    "\n",
    "#Color bar\n",
    "cax = plt.axes([0.95, 0.120, 0.03, 0.76])\n",
    "clb = plt.colorbar(cont,cax=cax)\n",
    "clb.ax.set_ylabel(r'\\textbf{Probability Real Data}', labelpad=30, rotation=270,fontsize=plt.rcParams['axes.labelsize'])\n",
    "clb.ax.tick_params(labelsize=18)\n",
    "\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=4,title='',markerscale=0.5,ncol=2,prop={'size':10})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two ways: we define the outliers as having a probability <0.50 or we look at the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_threshold = 0.50\n",
    "y_proba_train_rfc = rfc.predict_proba(XTrain)[:,1]\n",
    "y_proba_test_rfc = rfc.predict_proba(XTest)[:,1]\n",
    "y_proba_tot_rfc=rfc.predict_proba(points_tot)[:,1]\n",
    "\n",
    "Outlier_rfc=df[y_proba_tot_rfc<=proba_threshold]\n",
    "\n",
    "y_pred_train_rfc = rfc.predict(XTrain)\n",
    "y_pred_test_rfc = rfc.predict(XTest)\n",
    "y_pred_tot_rfc=rfc.predict(points_tot)\n",
    "\n",
    "Z_rfc = rfc.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "Z_rfc = Z_rfc.reshape(xx.shape)\n",
    "\n",
    "fig,ax1 = plt.subplots(figsize=(6,7), facecolor='w', edgecolor='k')\n",
    "ax1.plot(X_data,Y_data,'bo',alpha=0.5,ms=5,label=r'\\textbf{Data}',zorder=2)\n",
    "ax1.plot(X_out, Y_out,'ro',alpha=0.5,ms=5,label=r'\\textbf{Outliers}',zorder=2)\n",
    "ax1.plot(Outlier_rfc['X'], Outlier_rfc['Y'],color='g',marker='o',markerfacecolor='None',markeredgecolor='green',mew=2,markersize=8,linestyle='None',label=r'\\textbf{Outliers RF threshold}',zorder=3)\n",
    "ax1.plot(df['X'][y_pred_tot_rfc==-1], df['Y'][y_pred_tot_rfc==-1],color='k',marker='o',markerfacecolor='None',markeredgecolor='black',mew=2,markersize=14,linestyle='None',label=r'\\textbf{Outliers RF}',zorder=3)\n",
    "cont=ax1.contourf(xx, yy, Z_rfc,7,alpha=0.4,cmap=plt.cm.seismic) #note for contour values: cont.allsegs\n",
    "#Color bar\n",
    "cax = plt.axes([0.95, 0.120, 0.03, 0.76])\n",
    "clb = plt.colorbar(cont,cax=cax)\n",
    "clb.ax.set_ylabel(r'\\textbf{Probability}', labelpad=30, rotation=270,fontsize=plt.rcParams['axes.labelsize'])\n",
    "clb.ax.tick_params(labelsize=18)\n",
    "ax1.set_xlabel(r'\\textbf{X}')\n",
    "ax1.set_ylabel(r'\\textbf{Y}')\n",
    "ax1.legend(loc=4,title='',markerscale=0.5,ncol=2,prop={'size':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_out_rem=np.size(y_pred_tot_rfc[-50:][y_pred_tot_rfc[-50:]==-1])\n",
    "n_tot_out=np.size(y_pred_tot_rfc[y_pred_tot_rfc==-1])\n",
    "print('##### Random Forect Classifier  #####')\n",
    "\n",
    "print(r\"Using RF we remove %s/%s of the outliers \"%(n_out_rem,np.size(X_out)))\n",
    "print(r\"Using RF we remove %s/%s of the real data\"%((n_tot_out-n_out_rem),np.size(X_data)))\n",
    "\n",
    "print('######Classification metric for RF: ##########')\n",
    "\n",
    "print(f'Accuracy = {accuracy_score(target, y_pred_tot_rfc):.4f}')\n",
    "print(f'Precision = {precision_score(target, y_pred_tot_rfc):.4f}')\n",
    "print(f'Recall = {recall_score(target, y_pred_tot_rfc):.4f}')\n",
    "print(f'F1 score = {f1_score(target, y_pred_tot_rfc):.4f}')\n",
    "print(f'ROC AUC score = {roc_auc_score(target, y_pred_tot_rfc)*100:.4f}')\n",
    "\n",
    "\n",
    "n_out_rem=np.shape(pd.merge(Outlier_rfc, df[df['class']==-1], how='inner'))[0]\n",
    "n_tot_out=np.size(y_proba_tot_rfc[y_proba_tot_rfc<=proba_threshold])\n",
    "print('##### RF + threshold #####')\n",
    "print(r\"Using RF+threshold we remove %s/%s of the outliers \"%(n_out_rem,np.size(X_out)))\n",
    "print(r\"Using RF+threshold we remove %s/%s of the real data\"%((n_tot_out-n_out_rem),np.size(X_data)))\n",
    "print('###### Classification metric for RF+threshold: ##########')\n",
    "check_model(n_out_rem,np.size(X_out)-n_out_rem,(n_tot_out-n_out_rem),np.size(X_data)-(n_tot_out-n_out_rem))#TN FP FN TP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
